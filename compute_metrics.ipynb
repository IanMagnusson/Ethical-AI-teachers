{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompute pass@1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Qwen--Qwen2.5-7B-Instruct-1M\" # \"allenai--OLMo-2-1124-7B-Instruct\" #  # #   # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(x, subset=\"base\"):\n",
    "    if subset == \"base\":\n",
    "        return True if x['base_status'] == \"pass\" else False\n",
    "    elif subset == \"plus\":\n",
    "        return True if x['base_status'] == \"pass\" and x['plus_status'] == \"pass\" else False\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>solution</th>\n",
       "      <th>base_status</th>\n",
       "      <th>plus_status</th>\n",
       "      <th>base_fail_tests</th>\n",
       "      <th>plus_fail_tests</th>\n",
       "      <th>gt_solution</th>\n",
       "      <th>question_prompt</th>\n",
       "      <th>entry_point</th>\n",
       "      <th>baseline_base_correct</th>\n",
       "      <th>baseline_plus_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mbpp/103</td>\n",
       "      <td>def eulerian_num(n, m):\\n    # Base case\\n    ...</td>\n",
       "      <td>fail</td>\n",
       "      <td>fail</td>\n",
       "      <td>[[3, 1]]</td>\n",
       "      <td>[[1, 1]]</td>\n",
       "      <td>\\ndef eulerian_num(n, m): \\n\\tif (m &gt;= n or n ...</td>\n",
       "      <td>\"\"\"\\nWrite a function to find the Eulerian num...</td>\n",
       "      <td>eulerian_num</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Mbpp/106</td>\n",
       "      <td>def add_lists(list1, tuple1):\\n    # Convert t...</td>\n",
       "      <td>fail</td>\n",
       "      <td>fail</td>\n",
       "      <td>[[[5, 6, 7], [9, 10]]]</td>\n",
       "      <td>[[[1, [2, 3], {'a': 4}], [5, [6, 7], {'b': 8}]]]</td>\n",
       "      <td>\\ndef add_lists(test_list, test_tup):\\n  retur...</td>\n",
       "      <td>\"\"\"\\nWrite a function to append the given list...</td>\n",
       "      <td>add_lists</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Mbpp/109</td>\n",
       "      <td>def odd_Equivalents(binary_str, rotations):\\n ...</td>\n",
       "      <td>fail</td>\n",
       "      <td>fail</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>\\ndef odd_Equivalent(s,n): \\n    count=0\\n    ...</td>\n",
       "      <td>\"\"\"\\nWrite a python function to find the numbe...</td>\n",
       "      <td>odd_Equivalent</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mbpp/113</td>\n",
       "      <td>def check_integer(string):\\n    # Check if the...</td>\n",
       "      <td>pass</td>\n",
       "      <td>fail</td>\n",
       "      <td>[]</td>\n",
       "      <td>[['']]</td>\n",
       "      <td>\\ndef check_integer(text):\\n text = text.strip...</td>\n",
       "      <td>\"\"\"\\nWrite a function to check if a string rep...</td>\n",
       "      <td>check_integer</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Mbpp/123</td>\n",
       "      <td>def amicable_numbers_sum(n):\\n    # Helper fun...</td>\n",
       "      <td>fail</td>\n",
       "      <td>fail</td>\n",
       "      <td>[[999]]</td>\n",
       "      <td>[[10000]]</td>\n",
       "      <td>\\ndef div_sum(num):\\n    res = 1\\n    i = 2\\n ...</td>\n",
       "      <td>\"\"\"\\nWrite a function to sum all amicable numb...</td>\n",
       "      <td>amicable_numbers_sum</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     task_id                                           solution base_status  \\\n",
       "20  Mbpp/103  def eulerian_num(n, m):\\n    # Base case\\n    ...        fail   \n",
       "21  Mbpp/106  def add_lists(list1, tuple1):\\n    # Convert t...        fail   \n",
       "16  Mbpp/109  def odd_Equivalents(binary_str, rotations):\\n ...        fail   \n",
       "10  Mbpp/113  def check_integer(string):\\n    # Check if the...        pass   \n",
       "31  Mbpp/123  def amicable_numbers_sum(n):\\n    # Helper fun...        fail   \n",
       "\n",
       "   plus_status         base_fail_tests  \\\n",
       "20        fail                [[3, 1]]   \n",
       "21        fail  [[[5, 6, 7], [9, 10]]]   \n",
       "16        fail                      []   \n",
       "10        fail                      []   \n",
       "31        fail                 [[999]]   \n",
       "\n",
       "                                     plus_fail_tests  \\\n",
       "20                                          [[1, 1]]   \n",
       "21  [[[1, [2, 3], {'a': 4}], [5, [6, 7], {'b': 8}]]]   \n",
       "16                                                []   \n",
       "10                                            [['']]   \n",
       "31                                         [[10000]]   \n",
       "\n",
       "                                          gt_solution  \\\n",
       "20  \\ndef eulerian_num(n, m): \\n\\tif (m >= n or n ...   \n",
       "21  \\ndef add_lists(test_list, test_tup):\\n  retur...   \n",
       "16  \\ndef odd_Equivalent(s,n): \\n    count=0\\n    ...   \n",
       "10  \\ndef check_integer(text):\\n text = text.strip...   \n",
       "31  \\ndef div_sum(num):\\n    res = 1\\n    i = 2\\n ...   \n",
       "\n",
       "                                      question_prompt           entry_point  \\\n",
       "20  \"\"\"\\nWrite a function to find the Eulerian num...          eulerian_num   \n",
       "21  \"\"\"\\nWrite a function to append the given list...             add_lists   \n",
       "16  \"\"\"\\nWrite a python function to find the numbe...        odd_Equivalent   \n",
       "10  \"\"\"\\nWrite a function to check if a string rep...         check_integer   \n",
       "31  \"\"\"\\nWrite a function to sum all amicable numb...  amicable_numbers_sum   \n",
       "\n",
       "    baseline_base_correct  baseline_plus_correct  \n",
       "20                  False                  False  \n",
       "21                  False                  False  \n",
       "16                  False                  False  \n",
       "10                   True                  False  \n",
       "31                  False                  False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_file = f\"results/q_a_0/{model}_errors.csv\"\n",
    "baseline_df = pd.read_csv(baseline_file)\n",
    "baseline_df = baseline_df.sort_values(by='task_id')\n",
    "print(len(baseline_df))\n",
    "baseline_df['baseline_base_correct'] = baseline_df.apply(lambda x: is_correct(x, subset=\"base\"), axis=1)\n",
    "baseline_df['baseline_plus_correct'] = baseline_df.apply(lambda x: is_correct(x, subset=\"plus\"), axis=1)\n",
    "# print(baseline_df.baseline_base_correct.value_counts())\n",
    "# print(baseline_df.baseline_plus_correct.value_counts())\n",
    "baseline_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.30701754385964913, 0.0\n"
     ]
    }
   ],
   "source": [
    "# calculate baseline accuracy\n",
    "baseline_base_correct = baseline_df.baseline_base_correct.value_counts()[True]\n",
    "baseline_plus_correct = baseline_df.baseline_plus_correct.value_counts()[True] if True in baseline_df.baseline_plus_correct.value_counts() else 0\n",
    "baseline_base_accuracy = baseline_base_correct / len(baseline_df)\n",
    "baseline_plus_accuracy = baseline_plus_correct / len(baseline_df)\n",
    "print(f\"Baseline accuracy: {baseline_base_accuracy}, {baseline_plus_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mbpp_just_retrieval_as_feedback', 'mbpp_just_feedback_gpt-3.5-turbo_no_cheat_no_question', 'mbpp_with_feedback_gpt-3.5-turbo_no_cheat_no_question', 'mbpp_retrieval_as_feedback_gpt-3.5-turbo_no_cheat_no_question', 'mbpp_with_rag_teacher_feedback_gpt-3.5-turbo_no_cheat_no_question', 'mbpp_with_feedback_gpt-3.5-turbo', 'mbpp_just_feedback_rag_teacher_gpt-3.5-turbo_no_cheat_no_question', 'mbpp_just_feedback_gpt-3.5-turbo']\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# get all folders in results/a1\n",
    "folders = [f for f in os.listdir(\"results/a1\") if os.path.isdir(os.path.join(\"results/a1\", f))]\n",
    "print(folders)\n",
    "print(len(folders))\n",
    "output_file = f\"results/metrics/{model}_scores.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong to correct: 6 8\n",
      "mbpp_just_retrieval_as_feedback acc: 0.10846560846560846, 0.021164021164021163\n",
      "Wrong to correct: 5 10\n",
      "mbpp_just_feedback_gpt-3.5-turbo_no_cheat_no_question acc: 0.10582010582010581, 0.026455026455026454\n",
      "Wrong to correct: 10 10\n",
      "mbpp_with_feedback_gpt-3.5-turbo_no_cheat_no_question acc: 0.11904761904761904, 0.026455026455026454\n",
      "Wrong to correct: 18 21\n",
      "mbpp_retrieval_as_feedback_gpt-3.5-turbo_no_cheat_no_question acc: 0.1402116402116402, 0.05555555555555555\n",
      "Wrong to correct: 12 10\n",
      "mbpp_with_rag_teacher_feedback_gpt-3.5-turbo_no_cheat_no_question acc: 0.12433862433862433, 0.026455026455026454\n",
      "Wrong to correct: 25 17\n",
      "mbpp_with_feedback_gpt-3.5-turbo acc: 0.15873015873015872, 0.04497354497354497\n",
      "Wrong to correct: 5 4\n",
      "mbpp_just_feedback_rag_teacher_gpt-3.5-turbo_no_cheat_no_question acc: 0.10582010582010581, 0.010582010582010581\n",
      "Wrong to correct: 28 19\n",
      "mbpp_just_feedback_gpt-3.5-turbo acc: 0.16666666666666666, 0.05026455026455026\n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "for setup in folders:\n",
    "\n",
    "    results_file = f\"results/a1/{setup}/{model}_vllm_temp_0.0.eval_results.json\"\n",
    "    model_results = json.load(open(results_file))\n",
    "    results = [result[0] for result in model_results['eval'].values()]\n",
    "    exp_df = pd.DataFrame(results)\n",
    "\n",
    "    def get_baseline_correct(x, subset=\"base\"):\n",
    "        if x.task_id not in set(baseline_df.task_id):\n",
    "            return True\n",
    "        if subset == \"base\":\n",
    "            return baseline_df[baseline_df.task_id == x.task_id].baseline_base_correct.values[0]\n",
    "        elif subset == \"plus\":\n",
    "            return baseline_df[baseline_df.task_id == x.task_id].baseline_plus_correct.values[0]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    exp_df['baseline_base_correct'] = exp_df.apply(lambda x: get_baseline_correct(x, subset='base'), axis=1)\n",
    "    exp_df['baseline_plus_correct'] = exp_df.apply(lambda x: get_baseline_correct(x, subset='plus'), axis=1)\n",
    "\n",
    "    exp_df['base_correct'] = exp_df.apply(lambda x: is_correct(x, subset=\"base\"), axis=1)\n",
    "    exp_df['plus_correct'] = exp_df.apply(lambda x: is_correct(x, subset=\"plus\"), axis=1)\n",
    "    # print(exp_df.head())\n",
    "\n",
    "    # get the additional correct examples in exp_df that are wrong in baseline\n",
    "    exp_add_base_correct = len(exp_df[exp_df.base_correct & ~exp_df.baseline_base_correct])\n",
    "    exp_add_plus_correct = len(exp_df[exp_df.plus_correct & ~exp_df.baseline_plus_correct])\n",
    "    print(\"Wrong to correct:\", exp_add_base_correct, exp_add_plus_correct)\n",
    "\n",
    "    # check correct --> wrong\n",
    "    # base_correct_to_wrong = len(exp_df[~exp_df.base_correct &~exp_df.baseline_base_correct])\n",
    "    # plus_correct_to_wrong = len(exp_df[~exp_df.plus_correct &~exp_df.baseline_plus_correct])\n",
    "    # print(\"Correct to wrong:\", base_correct_to_wrong, plus_correct_to_wrong)\n",
    "\n",
    "    exp_base_accuracy = (baseline_base_correct + exp_add_base_correct) / len(exp_df)\n",
    "    exp_plus_accuracy = (baseline_plus_correct + exp_add_plus_correct) / len(exp_df)\n",
    "    print(f\"{setup} acc: {exp_base_accuracy}, {exp_plus_accuracy}\")\n",
    "\n",
    "    row = {'setup': setup, 'model': model, 'base_accuracy': exp_base_accuracy, 'plus_accuracy': exp_plus_accuracy}\n",
    "    # print(row)\n",
    "    all_rows.append(row)\n",
    "baseline_row = {'setup': 'baseline', 'model': model, 'base_accuracy': baseline_base_accuracy, 'plus_accuracy': baseline_plus_accuracy}\n",
    "all_rows.append(baseline_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setup</th>\n",
       "      <th>model</th>\n",
       "      <th>base_accuracy</th>\n",
       "      <th>plus_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mbpp_just_retrieval_as_feedback</td>\n",
       "      <td>Qwen--Qwen2.5-7B-Instruct-1M</td>\n",
       "      <td>0.108466</td>\n",
       "      <td>0.021164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mbpp_just_feedback_gpt-3.5-turbo_no_cheat_no_q...</td>\n",
       "      <td>Qwen--Qwen2.5-7B-Instruct-1M</td>\n",
       "      <td>0.105820</td>\n",
       "      <td>0.026455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mbpp_with_feedback_gpt-3.5-turbo_no_cheat_no_q...</td>\n",
       "      <td>Qwen--Qwen2.5-7B-Instruct-1M</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.026455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mbpp_retrieval_as_feedback_gpt-3.5-turbo_no_ch...</td>\n",
       "      <td>Qwen--Qwen2.5-7B-Instruct-1M</td>\n",
       "      <td>0.140212</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mbpp_with_rag_teacher_feedback_gpt-3.5-turbo_n...</td>\n",
       "      <td>Qwen--Qwen2.5-7B-Instruct-1M</td>\n",
       "      <td>0.124339</td>\n",
       "      <td>0.026455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               setup  \\\n",
       "0                    mbpp_just_retrieval_as_feedback   \n",
       "1  mbpp_just_feedback_gpt-3.5-turbo_no_cheat_no_q...   \n",
       "2  mbpp_with_feedback_gpt-3.5-turbo_no_cheat_no_q...   \n",
       "3  mbpp_retrieval_as_feedback_gpt-3.5-turbo_no_ch...   \n",
       "4  mbpp_with_rag_teacher_feedback_gpt-3.5-turbo_n...   \n",
       "\n",
       "                          model  base_accuracy  plus_accuracy  \n",
       "0  Qwen--Qwen2.5-7B-Instruct-1M       0.108466       0.021164  \n",
       "1  Qwen--Qwen2.5-7B-Instruct-1M       0.105820       0.026455  \n",
       "2  Qwen--Qwen2.5-7B-Instruct-1M       0.119048       0.026455  \n",
       "3  Qwen--Qwen2.5-7B-Instruct-1M       0.140212       0.055556  \n",
       "4  Qwen--Qwen2.5-7B-Instruct-1M       0.124339       0.026455  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save all rows to a csv file\n",
    "df = pd.DataFrame(all_rows)\n",
    "df.to_csv(output_file, index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute helpfulness, leakage frequency and non-leaking helpfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/pass_at_k_only_retried_instances/Qwen--Qwen2.5-7B-Instruct-1M_eval_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "output_file = f\"results/pass_at_k_only_retried_instances/{model}_eval_metrics.csv\"\n",
    "print(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_ids = [\n",
    "    \"Teacher Feedback\",\n",
    "    \"Teacher Feedback (No Cheat No Question)\",\n",
    "    \"Teacher Feedback with RAG\",\n",
    "    \"RAG Feedback\",\n",
    "]\n",
    "\n",
    "leak_exps = [\n",
    "    'mbpp_just_feedback_gpt-3.5-turbo',\n",
    "    'mbpp_just_feedback_gpt-3.5-turbo_no_cheat_no_question', \n",
    "    'mbpp_just_feedback_rag_teacher_gpt-3.5-turbo_no_cheat_no_question',\n",
    "    'mbpp_just_retrieval_as_feedback', \n",
    "    \n",
    "]\n",
    "\n",
    "actual_exps = [\n",
    "    'mbpp_with_feedback_gpt-3.5-turbo', \n",
    "    'mbpp_with_feedback_gpt-3.5-turbo_no_cheat_no_question', \n",
    "    'mbpp_with_rag_teacher_feedback_gpt-3.5-turbo_no_cheat_no_question',\n",
    "    'mbpp_retrieval_as_feedback_gpt-3.5-turbo_no_cheat_no_question', \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Feedback (actual = mbpp_with_feedback_gpt-3.5-turbo, leak exp = mbpp_just_feedback_gpt-3.5-turbo):\n",
      "base helpful: 0.316, leakage: 0.354, nonleak helpful: 0.013\n",
      "plus helpful: 0.149, leakage: 0.167, nonleak helpful: 0.009\n",
      "Teacher Feedback (No Cheat No Question) (actual = mbpp_with_feedback_gpt-3.5-turbo_no_cheat_no_question, leak exp = mbpp_just_feedback_gpt-3.5-turbo_no_cheat_no_question):\n",
      "base helpful: 0.127, leakage: 0.063, nonleak helpful: 0.127\n",
      "plus helpful: 0.088, leakage: 0.088, nonleak helpful: 0.079\n",
      "Teacher Feedback with RAG (actual = mbpp_with_rag_teacher_feedback_gpt-3.5-turbo_no_cheat_no_question, leak exp = mbpp_just_feedback_rag_teacher_gpt-3.5-turbo_no_cheat_no_question):\n",
      "base helpful: 0.152, leakage: 0.063, nonleak helpful: 0.127\n",
      "plus helpful: 0.088, leakage: 0.035, nonleak helpful: 0.079\n",
      "RAG Feedback (actual = mbpp_retrieval_as_feedback_gpt-3.5-turbo_no_cheat_no_question, leak exp = mbpp_just_retrieval_as_feedback):\n",
      "base helpful: 0.228, leakage: 0.076, nonleak helpful: 0.19\n",
      "plus helpful: 0.184, leakage: 0.07, nonleak helpful: 0.149\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_id</th>\n",
       "      <th>model</th>\n",
       "      <th>base_helpfulness</th>\n",
       "      <th>base_nonleak_helpfulness</th>\n",
       "      <th>base_leakage_freq</th>\n",
       "      <th>plus_helpfulness</th>\n",
       "      <th>plus_nonleak_helpfulness</th>\n",
       "      <th>plus_leakage_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Teacher Feedback</td>\n",
       "      <td>Qwen--Qwen2.5-7B-Instruct-1M</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Teacher Feedback (No Cheat No Question)</td>\n",
       "      <td>Qwen--Qwen2.5-7B-Instruct-1M</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Teacher Feedback with RAG</td>\n",
       "      <td>Qwen--Qwen2.5-7B-Instruct-1M</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RAG Feedback</td>\n",
       "      <td>Qwen--Qwen2.5-7B-Instruct-1M</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    exp_id                         model  \\\n",
       "0                         Teacher Feedback  Qwen--Qwen2.5-7B-Instruct-1M   \n",
       "1  Teacher Feedback (No Cheat No Question)  Qwen--Qwen2.5-7B-Instruct-1M   \n",
       "2                Teacher Feedback with RAG  Qwen--Qwen2.5-7B-Instruct-1M   \n",
       "3                             RAG Feedback  Qwen--Qwen2.5-7B-Instruct-1M   \n",
       "\n",
       "   base_helpfulness  base_nonleak_helpfulness  base_leakage_freq  \\\n",
       "0             0.316                     0.013              0.354   \n",
       "1             0.127                     0.127              0.063   \n",
       "2             0.152                     0.127              0.063   \n",
       "3             0.228                     0.190              0.076   \n",
       "\n",
       "   plus_helpfulness  plus_nonleak_helpfulness  plus_leakage_freq  \n",
       "0             0.149                     0.009              0.167  \n",
       "1             0.088                     0.079              0.088  \n",
       "2             0.088                     0.079              0.035  \n",
       "3             0.184                     0.149              0.070  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rows = []\n",
    "for exp_id, leak_exp, actual_exp in zip(exp_ids, leak_exps, actual_exps):\n",
    "    print(f\"{exp_id} (actual = {actual_exp}, leak exp = {leak_exp}):\")\n",
    "    \n",
    "    leak_exp_file = f\"results/a1/{leak_exp}/{model}_vllm_temp_0.0.eval_results.json\"\n",
    "    actual_exp_file = f\"results/a1/{actual_exp}/{model}_vllm_temp_0.0.eval_results.json\"\n",
    "\n",
    "    leak_exp_results = json.load(open(leak_exp_file))\n",
    "    actual_exp_results = json.load(open(actual_exp_file))\n",
    "\n",
    "    leak_exp_results = [result[0] for result in leak_exp_results['eval'].values()]\n",
    "    actual_exp_results = [result[0] for result in actual_exp_results['eval'].values()]\n",
    "\n",
    "    leak_exp_df = pd.DataFrame(leak_exp_results)\n",
    "    actual_exp_df = pd.DataFrame(actual_exp_results)\n",
    "\n",
    "    leak_exp_df['base_correct'] = leak_exp_df.apply(lambda x: is_correct(x, subset=\"base\"), axis=1)\n",
    "    leak_exp_df['plus_correct'] = leak_exp_df.apply(lambda x: is_correct(x, subset=\"plus\"), axis=1)\n",
    "\n",
    "    actual_exp_df['baseline_base_correct'] = actual_exp_df.apply(lambda x: get_baseline_correct(x, subset='base'), axis=1)\n",
    "    actual_exp_df['baseline_plus_correct'] = actual_exp_df.apply(lambda x: get_baseline_correct(x, subset='plus'), axis=1)\n",
    "\n",
    "    actual_exp_df['base_correct'] = actual_exp_df.apply(lambda x: is_correct(x, subset=\"base\"), axis=1)\n",
    "    actual_exp_df['plus_correct'] = actual_exp_df.apply(lambda x: is_correct(x, subset=\"plus\"), axis=1)\n",
    "\n",
    "    actual_exp_df['base_leaked'] = actual_exp_df.apply(lambda x: leak_exp_df[leak_exp_df.task_id == x.task_id].base_correct.values[0] & ~x.baseline_base_correct, axis=1)\n",
    "    actual_exp_df['plus_leaked'] = actual_exp_df.apply(lambda x: leak_exp_df[leak_exp_df.task_id == x.task_id].plus_correct.values[0] & ~x.baseline_plus_correct, axis=1)\n",
    "\n",
    "    # helpfulness \n",
    "    actual_exp_df['base_helpful'] = actual_exp_df.apply(lambda x: x.base_correct & ~x.baseline_base_correct, axis=1)\n",
    "    actual_exp_df['plus_helpful'] = actual_exp_df.apply(lambda x: x.plus_correct & ~x.baseline_plus_correct, axis=1)\n",
    "    # print(actual_exp_df.head())\n",
    "\n",
    "    # non leaking helpfulness \n",
    "    actual_exp_df['base_nonleak_helpful'] = actual_exp_df.apply(lambda x: x.base_correct & ~x.baseline_base_correct & ~x.base_leaked, axis=1)\n",
    "    actual_exp_df['plus_nonleak_helpful'] = actual_exp_df.apply(lambda x: x.plus_correct & ~x.baseline_plus_correct & ~x.plus_leaked, axis=1)\n",
    "\n",
    "    row = {\"exp_id\": exp_id, \"model\": model}\n",
    "\n",
    "    for subset in ['base', 'plus']:\n",
    "        total_cnt = len(actual_exp_df[actual_exp_df[f'baseline_{subset}_correct'] == 0])\n",
    "        helpful = round(len(actual_exp_df[actual_exp_df[f'{subset}_helpful'] == 1]) / total_cnt, 3)\n",
    "        nonleak_helpful = round(len(actual_exp_df[actual_exp_df[f'{subset}_nonleak_helpful'] == 1]) / total_cnt, 3)\n",
    "        leak_freq = round(len(actual_exp_df[actual_exp_df[f'{subset}_leaked'] == 1]) / total_cnt, 3)\n",
    "        print(f\"{subset} helpful: {helpful}, leakage: {leak_freq}, nonleak helpful: {nonleak_helpful}\")\n",
    "        row[f\"{subset}_helpfulness\"] = helpful\n",
    "        row[f\"{subset}_nonleak_helpfulness\"] = nonleak_helpful\n",
    "        row[f\"{subset}_leakage_freq\"] = leak_freq\n",
    "    \n",
    "   \n",
    "    all_rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(all_rows)\n",
    "df.to_csv(output_file, index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/pass_at_k_only_retried_instances/Qwen--Qwen2.5-7B-Instruct-1M_eval_metrics.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethical-ai-teachers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
