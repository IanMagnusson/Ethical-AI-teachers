{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompute pass@1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"allenai--OLMo-2-1124-7B-Instruct\" #  \"Qwen--Qwen2.5-7B-Instruct-1M\" #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(x, subset=\"base\"):\n",
    "    if subset == \"base\":\n",
    "        return True if x['base_status'] == \"pass\" else False\n",
    "    elif subset == \"plus\":\n",
    "        return True if x['base_status'] == \"pass\" and x['plus_status'] == \"pass\" else False\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>solution</th>\n",
       "      <th>base_status</th>\n",
       "      <th>plus_status</th>\n",
       "      <th>base_fail_tests</th>\n",
       "      <th>plus_fail_tests</th>\n",
       "      <th>baseline_base_correct</th>\n",
       "      <th>baseline_plus_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Mbpp/100</td>\n",
       "      <td>def next_smallest_palindrome(n):\\n    # Functi...</td>\n",
       "      <td>pass</td>\n",
       "      <td>pass</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Mbpp/101</td>\n",
       "      <td>def kth_element(arr, k):\\n    \"\"\"\\n    Functio...</td>\n",
       "      <td>pass</td>\n",
       "      <td>pass</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Mbpp/102</td>\n",
       "      <td>def snake_to_camel(snake_case_string):\\n    # ...</td>\n",
       "      <td>pass</td>\n",
       "      <td>fail</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[___python_program]]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Mbpp/103</td>\n",
       "      <td>def eulerian_num(n, m):\\n    \"\"\"\\n    Calculat...</td>\n",
       "      <td>fail</td>\n",
       "      <td>timeout</td>\n",
       "      <td>[[4, 1]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Mbpp/104</td>\n",
       "      <td>def sort_sublists(sublists):\\n    # Sort each ...</td>\n",
       "      <td>pass</td>\n",
       "      <td>pass</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      task_id                                           solution base_status  \\\n",
       "53   Mbpp/100  def next_smallest_palindrome(n):\\n    # Functi...        pass   \n",
       "55   Mbpp/101  def kth_element(arr, k):\\n    \"\"\"\\n    Functio...        pass   \n",
       "46   Mbpp/102  def snake_to_camel(snake_case_string):\\n    # ...        pass   \n",
       "114  Mbpp/103  def eulerian_num(n, m):\\n    \"\"\"\\n    Calculat...        fail   \n",
       "44   Mbpp/104  def sort_sublists(sublists):\\n    # Sort each ...        pass   \n",
       "\n",
       "    plus_status base_fail_tests        plus_fail_tests  baseline_base_correct  \\\n",
       "53         pass              []                     []                   True   \n",
       "55         pass              []                     []                   True   \n",
       "46         fail              []  [[___python_program]]                   True   \n",
       "114     timeout        [[4, 1]]                     []                  False   \n",
       "44         pass              []                     []                   True   \n",
       "\n",
       "     baseline_plus_correct  \n",
       "53                    True  \n",
       "55                    True  \n",
       "46                   False  \n",
       "114                  False  \n",
       "44                    True  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_file = f\"results/a0/mbpp/{model}_vllm_temp_0.0.eval_results.json\"\n",
    "baseline = json.load(open(baseline_file))\n",
    "baseline_results = [result[0] for result in baseline['eval'].values()]\n",
    "baseline_df = pd.DataFrame(baseline_results).sort_values(by='task_id')\n",
    "print(len(baseline_df))\n",
    "baseline_df['baseline_base_correct'] = baseline_df.apply(lambda x: is_correct(x, subset=\"base\"), axis=1)\n",
    "baseline_df['baseline_plus_correct'] = baseline_df.apply(lambda x: is_correct(x, subset=\"plus\"), axis=1)\n",
    "# print(baseline_df.baseline_base_correct.value_counts())\n",
    "# print(baseline_df.baseline_plus_correct.value_counts())\n",
    "baseline_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.43915343915343913, 0.36772486772486773\n"
     ]
    }
   ],
   "source": [
    "# calculate baseline accuracy\n",
    "baseline_base_correct = baseline_df.baseline_base_correct.value_counts()[True]\n",
    "baseline_plus_correct = baseline_df.baseline_plus_correct.value_counts()[True]\n",
    "baseline_base_accuracy = baseline_base_correct / len(baseline_df)\n",
    "baseline_plus_accuracy = baseline_plus_correct / len(baseline_df)\n",
    "print(f\"Baseline accuracy: {baseline_base_accuracy}, {baseline_plus_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mbpp_just_feedback_gpt-3.5-turbo', 'mbpp_just_retrieval_as_feedback', 'mbpp_retrieval_as_feedback_gpt-3.5-turbo_no_cheat_no_question', 'mbpp_just_feedback_rag_teacher_gpt-3.5-turbo_no_cheat_no_question', 'mbpp_just_feedback_gpt-3.5-turbo_no_cheat_no_question', 'mbpp_with_feedback_gpt-3.5-turbo_no_cheat_no_question', 'mbpp_with_feedback_gpt-3.5-turbo', 'mbpp_with_rag_teacher_feedback_gpt-3.5-turbo_no_cheat_no_question']\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# get all folders in results/a1\n",
    "folders = [f for f in os.listdir(\"results/a1\") if os.path.isdir(os.path.join(\"results/a1\", f))]\n",
    "print(folders)\n",
    "print(len(folders))\n",
    "output_file = f\"results/metrics/{model}_scores.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong to correct: 80 60\n",
      "mbpp_just_feedback_gpt-3.5-turbo acc: 0.6507936507936508, 0.5264550264550265\n",
      "Wrong to correct: 16 14\n",
      "mbpp_just_retrieval_as_feedback acc: 0.48148148148148145, 0.40476190476190477\n",
      "Wrong to correct: 12 13\n",
      "mbpp_retrieval_as_feedback_gpt-3.5-turbo_no_cheat_no_question acc: 0.4708994708994709, 0.4021164021164021\n",
      "Wrong to correct: 1 1\n",
      "mbpp_just_feedback_rag_teacher_gpt-3.5-turbo_no_cheat_no_question acc: 0.4417989417989418, 0.37037037037037035\n",
      "Wrong to correct: 4 6\n",
      "mbpp_just_feedback_gpt-3.5-turbo_no_cheat_no_question acc: 0.4497354497354497, 0.3835978835978836\n",
      "Wrong to correct: 19 16\n",
      "mbpp_with_feedback_gpt-3.5-turbo_no_cheat_no_question acc: 0.4894179894179894, 0.41005291005291006\n",
      "Wrong to correct: 70 54\n",
      "mbpp_with_feedback_gpt-3.5-turbo acc: 0.6243386243386243, 0.5105820105820106\n",
      "Wrong to correct: 21 20\n",
      "mbpp_with_rag_teacher_feedback_gpt-3.5-turbo_no_cheat_no_question acc: 0.4947089947089947, 0.42063492063492064\n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "for setup in folders:\n",
    "\n",
    "    results_file = f\"results/a1/{setup}/{model}_vllm_temp_0.0.eval_results.json\"\n",
    "    model_results = json.load(open(results_file))\n",
    "    results = [result[0] for result in model_results['eval'].values()]\n",
    "    exp_df = pd.DataFrame(results)\n",
    "\n",
    "    exp_df['baseline_base_correct'] = exp_df.apply(lambda x: baseline_df[baseline_df.task_id == x.task_id].baseline_base_correct.values[0], axis=1)\n",
    "    exp_df['baseline_plus_correct'] = exp_df.apply(lambda x: baseline_df[baseline_df.task_id == x.task_id].baseline_plus_correct.values[0], axis=1)\n",
    "\n",
    "    exp_df['base_correct'] = exp_df.apply(lambda x: is_correct(x, subset=\"base\"), axis=1)\n",
    "    exp_df['plus_correct'] = exp_df.apply(lambda x: is_correct(x, subset=\"plus\"), axis=1)\n",
    "    # print(exp_df.head())\n",
    "\n",
    "    # get the additional correct examples in exp_df that are wrong in baseline\n",
    "    exp_add_base_correct = len(exp_df[exp_df.base_correct & ~exp_df.baseline_base_correct])\n",
    "    exp_add_plus_correct = len(exp_df[exp_df.plus_correct & ~exp_df.baseline_plus_correct])\n",
    "    print(\"Wrong to correct:\", exp_add_base_correct, exp_add_plus_correct)\n",
    "\n",
    "    # check correct --> wrong\n",
    "    # base_correct_to_wrong = len(exp_df[~exp_df.base_correct &~exp_df.baseline_base_correct])\n",
    "    # plus_correct_to_wrong = len(exp_df[~exp_df.plus_correct &~exp_df.baseline_plus_correct])\n",
    "    # print(\"Correct to wrong:\", base_correct_to_wrong, plus_correct_to_wrong)\n",
    "\n",
    "    exp_base_accuracy = (baseline_base_correct + exp_add_base_correct) / len(exp_df)\n",
    "    exp_plus_accuracy = (baseline_plus_correct + exp_add_plus_correct) / len(exp_df)\n",
    "    print(f\"{setup} acc: {exp_base_accuracy}, {exp_plus_accuracy}\")\n",
    "\n",
    "    row = {'setup': setup, 'model': model, 'base_accuracy': exp_base_accuracy, 'plus_accuracy': exp_plus_accuracy}\n",
    "    # print(row)\n",
    "    all_rows.append(row)\n",
    "baseline_row = {'setup': 'baseline', 'model': model, 'base_accuracy': baseline_base_accuracy, 'plus_accuracy': baseline_plus_accuracy}\n",
    "all_rows.append(baseline_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setup</th>\n",
       "      <th>model</th>\n",
       "      <th>base_accuracy</th>\n",
       "      <th>plus_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mbpp_just_feedback_gpt-3.5-turbo</td>\n",
       "      <td>allenai--OLMo-2-1124-7B-Instruct</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0.526455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mbpp_just_retrieval_as_feedback</td>\n",
       "      <td>allenai--OLMo-2-1124-7B-Instruct</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.404762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mbpp_retrieval_as_feedback_gpt-3.5-turbo_no_ch...</td>\n",
       "      <td>allenai--OLMo-2-1124-7B-Instruct</td>\n",
       "      <td>0.470899</td>\n",
       "      <td>0.402116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mbpp_just_feedback_rag_teacher_gpt-3.5-turbo_n...</td>\n",
       "      <td>allenai--OLMo-2-1124-7B-Instruct</td>\n",
       "      <td>0.441799</td>\n",
       "      <td>0.370370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mbpp_just_feedback_gpt-3.5-turbo_no_cheat_no_q...</td>\n",
       "      <td>allenai--OLMo-2-1124-7B-Instruct</td>\n",
       "      <td>0.449735</td>\n",
       "      <td>0.383598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               setup  \\\n",
       "0                   mbpp_just_feedback_gpt-3.5-turbo   \n",
       "1                    mbpp_just_retrieval_as_feedback   \n",
       "2  mbpp_retrieval_as_feedback_gpt-3.5-turbo_no_ch...   \n",
       "3  mbpp_just_feedback_rag_teacher_gpt-3.5-turbo_n...   \n",
       "4  mbpp_just_feedback_gpt-3.5-turbo_no_cheat_no_q...   \n",
       "\n",
       "                              model  base_accuracy  plus_accuracy  \n",
       "0  allenai--OLMo-2-1124-7B-Instruct       0.650794       0.526455  \n",
       "1  allenai--OLMo-2-1124-7B-Instruct       0.481481       0.404762  \n",
       "2  allenai--OLMo-2-1124-7B-Instruct       0.470899       0.402116  \n",
       "3  allenai--OLMo-2-1124-7B-Instruct       0.441799       0.370370  \n",
       "4  allenai--OLMo-2-1124-7B-Instruct       0.449735       0.383598  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save all rows to a csv file\n",
    "df = pd.DataFrame(all_rows)\n",
    "df.to_csv(output_file, index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute helpfulness, leakage frequency and non-leaking helpfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/metrics/allenai--OLMo-2-1124-7B-Instruct_eval_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "output_file = f\"results/metrics/{model}_eval_metrics.csv\"\n",
    "print(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_ids = [\n",
    "    \"Teacher Feedback\",\n",
    "    \"Teacher Feedback (No Cheat No Question)\",\n",
    "    \"Teacher Feedback with RAG\",\n",
    "    \"RAG Feedback\",\n",
    "]\n",
    "\n",
    "leak_exps = [\n",
    "    'mbpp_just_feedback_gpt-3.5-turbo',\n",
    "    'mbpp_just_feedback_gpt-3.5-turbo_no_cheat_no_question', \n",
    "    'mbpp_just_feedback_rag_teacher_gpt-3.5-turbo_no_cheat_no_question',\n",
    "    'mbpp_just_retrieval_as_feedback', \n",
    "    \n",
    "]\n",
    "\n",
    "actual_exps = [\n",
    "    'mbpp_with_feedback_gpt-3.5-turbo', \n",
    "    'mbpp_with_feedback_gpt-3.5-turbo_no_cheat_no_question', \n",
    "    'mbpp_with_rag_teacher_feedback_gpt-3.5-turbo_no_cheat_no_question',\n",
    "    'mbpp_retrieval_as_feedback_gpt-3.5-turbo_no_cheat_no_question', \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Feedback (actual = mbpp_with_feedback_gpt-3.5-turbo, leak exp = mbpp_just_feedback_gpt-3.5-turbo):\n",
      "base helpful: 0.185, leakage: 0.627, nonleak helpful: 0.0\n",
      "plus helpful: 0.143, leakage: 0.526, nonleak helpful: 0.0\n",
      "Teacher Feedback (No Cheat No Question) (actual = mbpp_with_feedback_gpt-3.5-turbo_no_cheat_no_question, leak exp = mbpp_just_feedback_gpt-3.5-turbo_no_cheat_no_question):\n",
      "base helpful: 0.05, leakage: 0.384, nonleak helpful: 0.0\n",
      "plus helpful: 0.042, leakage: 0.384, nonleak helpful: 0.0\n",
      "Teacher Feedback with RAG (actual = mbpp_with_rag_teacher_feedback_gpt-3.5-turbo_no_cheat_no_question, leak exp = mbpp_just_feedback_rag_teacher_gpt-3.5-turbo_no_cheat_no_question):\n",
      "base helpful: 0.056, leakage: 0.376, nonleak helpful: 0.0\n",
      "plus helpful: 0.053, leakage: 0.37, nonleak helpful: 0.0\n",
      "RAG Feedback (actual = mbpp_retrieval_as_feedback_gpt-3.5-turbo_no_cheat_no_question, leak exp = mbpp_just_retrieval_as_feedback):\n",
      "base helpful: 0.032, leakage: 0.413, nonleak helpful: 0.0\n",
      "plus helpful: 0.034, leakage: 0.405, nonleak helpful: 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_id</th>\n",
       "      <th>model</th>\n",
       "      <th>base_helpfulness</th>\n",
       "      <th>base_nonleak_helpfulness</th>\n",
       "      <th>base_leakage_freq</th>\n",
       "      <th>plus_helpfulness</th>\n",
       "      <th>plus_nonleak_helpfulness</th>\n",
       "      <th>plus_leakage_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Teacher Feedback</td>\n",
       "      <td>allenai--OLMo-2-1124-7B-Instruct</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Teacher Feedback (No Cheat No Question)</td>\n",
       "      <td>allenai--OLMo-2-1124-7B-Instruct</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Teacher Feedback with RAG</td>\n",
       "      <td>allenai--OLMo-2-1124-7B-Instruct</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RAG Feedback</td>\n",
       "      <td>allenai--OLMo-2-1124-7B-Instruct</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    exp_id                             model  \\\n",
       "0                         Teacher Feedback  allenai--OLMo-2-1124-7B-Instruct   \n",
       "1  Teacher Feedback (No Cheat No Question)  allenai--OLMo-2-1124-7B-Instruct   \n",
       "2                Teacher Feedback with RAG  allenai--OLMo-2-1124-7B-Instruct   \n",
       "3                             RAG Feedback  allenai--OLMo-2-1124-7B-Instruct   \n",
       "\n",
       "   base_helpfulness  base_nonleak_helpfulness  base_leakage_freq  \\\n",
       "0             0.185                       0.0              0.627   \n",
       "1             0.050                       0.0              0.384   \n",
       "2             0.056                       0.0              0.376   \n",
       "3             0.032                       0.0              0.413   \n",
       "\n",
       "   plus_helpfulness  plus_nonleak_helpfulness  plus_leakage_freq  \n",
       "0             0.143                       0.0              0.526  \n",
       "1             0.042                       0.0              0.384  \n",
       "2             0.053                       0.0              0.370  \n",
       "3             0.034                       0.0              0.405  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rows = []\n",
    "for exp_id, leak_exp, actual_exp in zip(exp_ids, leak_exps, actual_exps):\n",
    "    print(f\"{exp_id} (actual = {actual_exp}, leak exp = {leak_exp}):\")\n",
    "    \n",
    "    leak_exp_file = f\"results/a1/{leak_exp}/{model}_vllm_temp_0.0.eval_results.json\"\n",
    "    actual_exp_file = f\"results/a1/{actual_exp}/{model}_vllm_temp_0.0.eval_results.json\"\n",
    "\n",
    "    leak_exp_results = json.load(open(leak_exp_file))\n",
    "    actual_exp_results = json.load(open(actual_exp_file))\n",
    "\n",
    "    leak_exp_results = [result[0] for result in leak_exp_results['eval'].values()]\n",
    "    actual_exp_results = [result[0] for result in actual_exp_results['eval'].values()]\n",
    "\n",
    "    leak_exp_df = pd.DataFrame(leak_exp_results)\n",
    "    actual_exp_df = pd.DataFrame(actual_exp_results)\n",
    "\n",
    "\n",
    "    leak_exp_df['base_correct'] = leak_exp_df.apply(lambda x: is_correct(x, subset=\"base\"), axis=1)\n",
    "    leak_exp_df['plus_correct'] = leak_exp_df.apply(lambda x: is_correct(x, subset=\"plus\"), axis=1)\n",
    "\n",
    "\n",
    "    actual_exp_df['baseline_base_correct'] = actual_exp_df.apply(lambda x: baseline_df[baseline_df.task_id == x.task_id].baseline_base_correct.values[0], axis=1)\n",
    "    actual_exp_df['baseline_plus_correct'] = actual_exp_df.apply(lambda x: baseline_df[baseline_df.task_id == x.task_id].baseline_plus_correct.values[0], axis=1)\n",
    "\n",
    "    actual_exp_df['base_correct'] = actual_exp_df.apply(lambda x: is_correct(x, subset=\"base\"), axis=1)\n",
    "    actual_exp_df['plus_correct'] = actual_exp_df.apply(lambda x: is_correct(x, subset=\"plus\"), axis=1)\n",
    "\n",
    "    actual_exp_df['base_leaked'] = actual_exp_df.apply(lambda x: leak_exp_df[leak_exp_df.task_id == x.task_id].base_correct.values[0], axis=1)\n",
    "    actual_exp_df['plus_leaked'] = actual_exp_df.apply(lambda x: leak_exp_df[leak_exp_df.task_id == x.task_id].plus_correct.values[0], axis=1)\n",
    "\n",
    "    # helpfulness \n",
    "    actual_exp_df['base_helpful'] = actual_exp_df.apply(lambda x: x.base_correct & ~x.baseline_base_correct, axis=1)\n",
    "    actual_exp_df['plus_helpful'] = actual_exp_df.apply(lambda x: x.plus_correct & ~x.baseline_plus_correct, axis=1)\n",
    "\n",
    "    # print(actual_exp_df.head())\n",
    "\n",
    "    # non leaking helpfulness \n",
    "    actual_exp_df['base_nonleak_helpful'] = actual_exp_df.apply(lambda x: x.base_correct and ~x.baseline_base_correct and ~x.base_leaked, axis=1)\n",
    "    actual_exp_df['plus_nonleak_helpful'] = actual_exp_df.apply(lambda x: x.plus_correct and ~x.baseline_plus_correct and ~x.plus_leaked, axis=1)\n",
    "\n",
    "    row = {\"exp_id\": exp_id, \"model\": model}\n",
    "    for subset in ['base', 'plus']:\n",
    "        helpful = round(len(actual_exp_df[actual_exp_df[f'{subset}_helpful'] == 1]) / len(actual_exp_df), 3)\n",
    "        nonleak_helpful = round(len(actual_exp_df[actual_exp_df[f'{subset}_nonleak_helpful'] == 1]) / len(actual_exp_df), 3)\n",
    "        leak_freq = round(len(actual_exp_df[actual_exp_df[f'{subset}_leaked'] == 1]) / len(actual_exp_df), 3)\n",
    "        print(f\"{subset} helpful: {helpful}, leakage: {leak_freq}, nonleak helpful: {nonleak_helpful}\")\n",
    "        row[f\"{subset}_helpfulness\"] = helpful\n",
    "        row[f\"{subset}_nonleak_helpfulness\"] = nonleak_helpful\n",
    "        row[f\"{subset}_leakage_freq\"] = leak_freq\n",
    "    \n",
    "   \n",
    "    all_rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(all_rows)\n",
    "df.to_csv(output_file, index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vidcap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
